
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Xi Wang">

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Architects+Daughter" rel="stylesheet" type="text/css">
<!--    <link href="css/font-awesome.min.css" rel="stylesheet" type="text/css">-->
      <link rel="stylesheet" href="https://i.icomoon.io/public/temp/2024dcaef0/UntitledProject/style.css">
<!--    <link href="css/mystyle.css" rel="stylesheet">-->

    <!-- Google Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-61168812-1', 'auto');
    ga('send', 'pageview');

    </script>
    <!-- End Google Analytics -->

  </head>

  <body>

    <!-- Fixed navbar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#about">About</a></li>
              <li><a href="#news">News</a></li>
            <li><a href="#pubs">Publications</a></li>
            <li><a href="#personal">Personal</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">
        <div class="row" id="about">
            <div class="span12">
                <h2>Xi Wang</h2>
            </div>
        </div><br>
        <div class="row featurette", id="about">
        	<div class="col-md-10">
        		<p>xi.wang (at) inf.ethz.ch</p>
        		<br>
		        <p>
                    I am an ETH Postdoc Fellow in the <a href="https://ait.ethz.ch/">Advanced Interactive Technologies lab</a> at ETH, Zurich with <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
                    and a Postdoc researcher in the <a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a> at ETH, Zurich with <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjcxLC0xOTcxNDY1MTc4.html">Luc Van Gool</a>.
                    I completed my PhD in the <a href="http://www.cg.tu-berlin.de">Computer Graphics group</a> at TU Berlin, advised by <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Prof. Marc Alexa</a>.
                    In 2020, I visited MIT working in the <a href="http://olivalab.mit.edu/">Computational Perception & Cognition Group</a> led by <a href="http://olivalab.mit.edu/audeoliva.html">Aude Oliva</a>.
                    Later that year I interned at <a href="https://research.adobe.com/">Adobe Research</a> working with
                    <a href="https://research.adobe.com/person/zoya-bylinskii/">Zoya Bylinskii</a> and <a href="http://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a>.
                    <br>
                    <br>
                    My research interests fall at the intersection of computer vision & graphics, and vision science.
                    My goal is to <b>bring human common sense and behavior patterns into machine learning</b>.
                    During my Ph.D., I have studied how humans perceive 3D shapes and what we can tell about people's mental imagery through observations of their eye movements.
                    My current research interests are <b>vision-language multimodal learning</b>, with a focus on understanding <b>how humans' intent drives their actions</b> and <b>their interactions with the surroundings</b>.
                    I am excited to learn about human behavior patterns and to leverage the gained knowledge in computational models and applications.
                <p>

                <p class="text-justify">
        			&nbsp<span class="icon-profile"></span>  <a href="files/cv_xi_wang_web.pdf">Curriculum vitae</a>  &nbsp;&nbsp;/&nbsp;&nbsp;
                    <span class="icon-googlescholar"></span>  <a href="https://scholar.google.de/citations?user=IYUPj9MAAAAJ&hl=en&oi=ao"><i class="ai ai-google-scholar" aria-hidden="false" title="Google Scholar"></i>Scholar profile</a>  &nbsp;&nbsp;/&nbsp;&nbsp;
                    <span class="icon-twitter1"></span>  <a href="https://twitter.com/xiwang1212"></i>Twitter</a>
      			</p>
        	</div>
        		
        	<div class="col-md-2">
          		<img src="imgs/xi.jpg" class="featurette-image img-responsive center-block"  alt="Xi Wang" style="width: 220px"></img>
        	</div>
		</div>

        <!--******************************************************-->
        <hr class="featurette-divider"  id="news"/>
        <div class='featurette-heading'>
            <h3>News</h3>
        </div>
        <br>
        <div class="row">
            <div class="col-md-3">
                <h5>February 2023</h5>
            </div>
            <div class="col-md-9">
                Our paper <a href="https://psyarxiv.com/nd653">"A computational approach to studying aesthetic judgments of ambiguous artworks"</a> is
                accepted to <a href="https://www.apa.org/pubs/journals/aca">Psychology of Aesthetics, Creativity and the Arts</a>.
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-md-3">
                <h5>February 2023</h5>
            </div>
            <div class="col-md-9">
                I join the <a href="https://vision.ee.ethz.ch/">Computer Vision lab</a> led by
                <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjcxLC0xOTcxNDY1MTc4.html">Prof. Luc Van Gool</a> at ETH Zurich.
                We will be working on projects related to eye tracking and egocentric video understanding. If you want to talk more about them, reach out!
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <h5>January 2023</h5>
            </div>
            <div class="col-md-9">
                Our Rhobin workshop on human-object interaction reconstruction at CVPR'23 is <a href="https://rhobin-challenge.github.io/">online</a>.
                We have an amazing line-up of speakers. Check out the details and consider submitting a paper!
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <h5>December 2022</h5>
            </div>
            <div class="col-md-9">
                My <a href="https://ethz.ch/en/research/research-promotion/seed-projects.html">ETH Career Seed Award</a> application has been approved. Looking forward to exploring new ideas that are enabled by the grant.
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <h5>December 2022</h5>
            </div>
            <div class="col-md-9">
                Our workshop "First Rhobin Challenge - Reconstruction of human-object interaction" is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>. Stay tuned for more updates.
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <h5>September 2022</h5>
            </div>
            <div class="col-md-9">
                I present our work on <a href="https://arxiv.org/abs/2209.02485">"Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors"</a> at <a href="https://3dvconf.github.io/2022/">3DV in Prague</a>.
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-3">
                <h5>August 2022</h5>
            </div>
            <div class="col-md-9">
                I will serve as a Sponsor Chair for <a href="https://etra.acm.org/2022/">the ACM Symposium on Eye Tracking Research and Applications (ETRA2023)</a>.
            </div>
        </div>
        <br>

<!--           <div class="row">-->
            <!--            <div class="col-md-3">-->
            <!--                <h5>March 2022</h5>-->
            <!--            </div>-->
            <!--            <div class="col-md-9">-->
            <!--                I participate in the Career Seed Panel to evaluate the <a href="https://ethz.ch/en/research/research-promotion/seed-projects.html"> ETH Career Seed Awards</a>.-->
            <!--            </div>-->
            <!--        </div>-->
            <!--        <br>-->

<!--        <div class="row">-->
<!--            <div class="col-md-3">-->
<!--                <h5>July 2021</h5>-->
<!--            </div>-->
<!--            <div class="col-md-9">-->
<!--                Our paper <a href="https://arxiv.org/abs/2106.05953">"Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning"</a> is-->
<!--                accepted at <a href="https://iccv2021.thecvf.com/home">ICCV 2021</a> as an oral presentation.-->
<!--            </div>-->
<!--        </div>-->
<!--        <br>-->

<!--        <div class="row">-->
<!--            <div class="col-md-3">-->
<!--                <h5>May 2021</h5>-->
<!--            </div>-->
<!--            <div class="col-md-9">-->
<!--                I present our <a href="./tap-project-page/files/Toward-quantifying-ambiguities-in-artistic-images.pdf">poster</a> on-->
<!--                "Toward Quantifying Ambiguities in Artistic Images" at the <a href="https://www.visionsciences.org/">virtual VSS 2021 meeting</a>.-->
<!--            </div>-->
<!--        </div>-->
<!--        <br>-->

<!--        <div class="row">-->
<!--            <div class="col-md-3">-->
<!--                <h5>May 2021</h5>-->
<!--            </div>-->
<!--            <div class="col-md-9">-->
<!--                We have successfully hosted the <a href="https://emics-2021.github.io/EMICS/">EMICS virtual workshop</a>-->
<!--                at CHI'21. All invited speakers’ talks are online available <a href="https://www.youtube.com/playlist?list=PL1Lg6TfQiU5QQA1s6RB_WKY4eEUNWJ4vb">here</a>-->
<!--                as well as <a href="https://www.youtube.com/playlist?list=PL1Lg6TfQiU5QSi6TrteURaGr7huRUB-p0">spotlight presentations</a>.-->
<!--                Visit our workshop page and find out more details.-->
<!--            </div>-->
<!--        </div>-->
<!--        <br>-->

<!--        <div class="row">-->
<!--            <div class="col-md-3">-->
<!--                <h5>March 2021</h5>-->
<!--            </div>-->
<!--            <div class="col-md-9">-->
<!--                I join the <a href="https://ait.ethz.ch/">Advanced Interactive Technologies lab</a> led by-->
<!--                <a href="https://ait.ethz.ch/people/hilliges/">Prof. Otmar Hilliges</a> at ETH Zurich as a postdoc.-->
<!--                We will be working on projects related to real-world activities and eye tracking. If you want to talk more-->
<!--                about them, reach out!-->
<!--            </div>-->
<!--        </div>-->
<!--        <br>-->

<!--        <div class="row">-->
<!--            <div class="col-md-3">-->
<!--                <h5>December 2020</h5>-->
<!--            </div>-->
<!--            <div class="col-md-9">-->
<!--                My application for the ETH Zurich Postdoctoral Fellowship has been approved by the Research Commission.-->
<!--                Looking forward to join the <a href="https://ait.ethz.ch/">AIT</a> group!-->
<!--            </div>-->
<!--        </div>-->


        <!--******************************************************-->
    	<hr class="featurette-divider"  id="pubs"/>
		<h3 class="featurette-heading">Publication</h3>
		<br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./imgs/paca.png" alt="Gallery curation task" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    A Computational Approach to Studying Aesthetic Judgments of Ambiguous Artworks. <small>Psychology of Aesthetics, Creativity and the Arts, 2023</small>
                </h4>
                <b>Xi Wang</b>, <a href="http://web.mit.edu/zoya/www/">Zoya Bylinskii</a>,
                <a href="http://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a>, and
                <a href="https://robertpepperell.com/">Robert Pepperell</a>
                <br>
                <a href="./PACA-proj-page/">Project page</a>&nbsp;&nbsp;&nbsp;
                <a href="https://psyarxiv.com/nd653">PDF</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./imgs/transfusion.png" alt="transfusion" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation. <small>arXiv 2022</small>
                </h4>
                Razvan-George Pasca,
                Alexey Gavryushin,
                <a href="https://yenlingkuo.com/">Yen-Ling Kuo</a>,
                <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>, and
                <b>Xi Wang</b>
                <br>
                <a href="https://eth-ait.github.io/transfusion-proj/">Project page</a>&nbsp;&nbsp;&nbsp;
                <a href="https://arxiv.org/abs/2301.09209">PDF</a>&nbsp;&nbsp;&nbsp;
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./imgs/gazenerf.png" alt="gazenerf" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields. <small>CVPR 2023</small>
                </h4>
                Alessandro Ruzzi*,
                Xiangwei Shi*,
                <b>Xi Wang</b>,
                <a href="https://ait.ethz.ch/people/lig/">Gengyan Li</a>,
                <a href="https://research.nvidia.com/person/shalini-de-mello">Shalini De Mello</a>,
                <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>,
                <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a>, and
                <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
                <br>
                <a href="https://arxiv.org/abs/2212.04823">PDF</a>&nbsp;&nbsp;&nbsp;
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./imgs/deepfake-caricatures.png" alt="deepfake" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines. <small>arXiv 2022</small>
                </h4>
                <a href="https://camilofosco.com/">Camilo Fosco*</a>,
                <a href="https://www.emiliejosephs.com/">Emilie Josephs*</a>,
                <a href="https://www.alexandonian.com/">Alex Andonian</a>,
                Allen Lee,
                <b>Xi Wang</b>, and
                <a href="http://olivalab.mit.edu/audeoliva.html">Aude Oliva</a>
                <br>
                <a href="https://camilofosco.com/deepfake_caricatures_website/">Project page</a>&nbsp;&nbsp;&nbsp;
                <a href="https://arxiv.org/abs/2206.00535">PDF</a>&nbsp;&nbsp;&nbsp;
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./imgs/3dv22.png" alt="rhoi" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors. <small>3DV 2022</small>
                </h4>
                <b>Xi Wang*</b>,
                Gen Li*,
                <a href="https://yenlingkuo.com/">Yen-Ling Kuo</a>,
                <a href="https://ps.is.mpg.de/person/mkocabas">Muhammed Kocabas</a>,
                <a href="https://ait.ethz.ch/people/eaksan/">Emre Aksan</a>, and
                <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
                <br>
                <a href="https://eth-ait.github.io/rhoi/">Project page</a>&nbsp;&nbsp;&nbsp;
                <a href="https://arxiv.org/abs/2209.02485">PDF</a>&nbsp;&nbsp;&nbsp;
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./imgs/peclr.gif" alt="peclr" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. <small>ICCV 2021, oral</small>
                </h4>
                <a href="https://ait.ethz.ch/people/spurra/">Adrian Spurr*</a>,
                Aneesh Dahiya*,
                <b>Xi Wang</b>,
                <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a>, and
                <a href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a>
                <br>
                <a href="https://ait.ethz.ch/projects/2021/peclr/">Project page</a>&nbsp;&nbsp;&nbsp;
                <a href="https://arxiv.org/abs/2106.05953">PDF</a>&nbsp;&nbsp;&nbsp;
                <a href="https://github.com/dahiyaaneesh/peclr">Code and Models</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./imgs/vss2021.jpg" alt="vss2021" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Toward Quantifying Ambiguities in Artistic Images. <small>VSS 2021</small>
                </h4>
                <b>Xi Wang</b>, <a href="http://web.mit.edu/zoya/www/">Zoya Bylinskii</a>,
                <a href="http://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a>, and
                <a href="https://robertpepperell.com/">Robert Pepperell</a>
                <br>
                <a href="./tap-project-page/">Project page</a>&nbsp;&nbsp;&nbsp;
                <a href="tap-project-page/files/Toward-quantifying-ambiguities-in-artistic-images.pdf">Poster</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/chi2021.png" alt="chi2021" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    EMICS'21: Eye Movements as an Interface to Cognitive State <small>CHI'21 Workshop</small>
                </h4>
                <b>Xi Wang</b>, <a href="http://web.mit.edu/zoya/www/">Zoya Bylinskii</a>,
                <a href="https://www.queensu.ca/psychology/people/faculty/monica-castelhano">Monica Castelhano</a>,
                <a href="https://www.linkedin.com/in/jameshillis/">James Hillis</a>, and
                <a href="http://andrewd.ces.clemson.edu/">Andrew T. Duchowski</a>
                <br>
                <a href="https://emics-2021.github.io/EMICS/">Event page</a>&nbsp;&nbsp;&nbsp;
            </div>
        </div>
        <p></p>
        <br>


        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./imgs/brm2021.png" alt="brm2021" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    A consensus-based elastic matching algorithm for mapping recall fixations onto encoding fixations in the looking-at-nothing paradigm. <small>Behavior Research Methods, 2021</small>
                </h4>
                <b>Xi Wang</b>, <a href="https://www.researchgate.net/profile/Kenneth_Holmqvist">Kenneth Holmqvist</a>, and
                <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a>
                <br>
                <a href="./project_mental_imagery/em.html">Project page</a> &nbsp;&nbsp;&nbsp;
                <a href="https://link.springer.com/article/10.3758/s13428-020-01513-1">PDF</a> &nbsp;&nbsp;&nbsp;<a href="./files/bib/brm21.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="./tap-project-page/files/teaser-02.jpg" alt="chi2020" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Toward Quantifying Ambiguities in Artistic Images. <small>TAP, 2020</small>
                </h4>
                <b>Xi Wang</b>, <a href="http://web.mit.edu/zoya/www/">Zoya Bylinskii</a>,
                <a href="http://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a>, and
                <a href="https://robertpepperell.com/">Robert Pepperell</a>
                <br>
                <a href="./tap-project-page/">Project page</a>&nbsp;&nbsp;&nbsp;
                <a href="https://arxiv.org/abs/2008.09688">arXiv</a> &nbsp;&nbsp;&nbsp;<a href="./tap-project-page/files/bib.txt">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/sr20.png" alt="sr2020" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Computational discrimination between natural images based on gaze during mental
                    imagery <small>Scientific Reports, 2020</small>
                </h4>
                <b>Xi Wang</b>, <a href="https://www.cv.tu-berlin.de/menue/mitarbeiter/andreas_ley/">Andreas Ley</a>,
                <a href="http://www.cg.tu-berlin.de/team/sebastian-koch/">Sebastian Koch</a>,
                <a href="https://www.cc.gatech.edu/~hays/">James Hays</a>, <a href="https://www.researchgate.net/profile/Kenneth_Holmqvist">Kenneth Holmqvist</a>, and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <br>
                <a href="./project_mental_imagery/similarity.html">Project page</a> &nbsp;&nbsp;&nbsp;
                <a href="https://www.nature.com/articles/s41598-020-69807-0">PDF</a> &nbsp;&nbsp;&nbsp;<a href="files/bib/sr20.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/chi20.png" alt="chi2020" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    EMICS'20: Eye Movements as an Interface to Cognitive State <small>CHI SIG, 2020</small>
                </h4>
                <b>Xi Wang</b>, <a href="http://web.mit.edu/zoya/www/">Zoya Bylinskii</a>,
                <a href="https://www.queensu.ca/psychology/people/faculty/monica-castelhano">Monica Castelhano</a>,
                <a href="https://www.linkedin.com/in/jameshillis/">James Hillis</a>, and
                <a href="http://andrewd.ces.clemson.edu/">Andrew T. Duchowski</a>
                <br>
                <a href="https://emics-2020.github.io/EMICS/">Event page</a>&nbsp;&nbsp;&nbsp;
                <a href="files/papers/emics.pdf">PDF</a> &nbsp;&nbsp;&nbsp;<a href="files/bib/emics.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/hmd.png" alt="Depth-based Dynamic Adjustment of Rendering for Head-mounted Displays Decreases Visual Comfort" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Keep It Simple: Depth-based Dynamic Adjustment of Rendering for Head-mounted Displays Decreases Visual Comfort <small>TAP, 2019</small>
                </h4>
                Jochen Jacobs, <b>Xi Wang</b>, and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <a href="files/papers/keepitsimple.pdf">PDF</a> &nbsp;&nbsp;&nbsp;<a href="files/bib/keepitsimple.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/annulus.png" alt="Center of circle after perspective transformation" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Center of circle after perspective transformation <small>arXiv, 2019</small>
                </h4>
                <b>Xi Wang</b>, <a href="https://page.math.tu-berlin.de/~chern/">Albert Chern</a>, and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <a href="files/papers/concentriccircles.pdf">PDF</a> &nbsp;&nbsp;&nbsp;<a href="files/bib/cc19.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/vergencev.png" alt="The mean point of vergence is biased under projection" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    The mean point of vergence is biased under projection <small>JEMR, 2019</small>
                </h4>
                <b>Xi Wang</b>, <a href="https://www.researchgate.net/profile/Kenneth_Holmqvist">Kenneth Holmqvist</a>, and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>

                <a href="files/papers/meanvergence.pdf">PDF</a> &nbsp;&nbsp;&nbsp;<a href="files/bib/jemr19.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/chi19.png" alt="Tracking the Gaze on Objects in 3D" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    The Mental Image Revealed by Gaze Tracking <small>CHI, 2019</small>
                </h4>
                <b>Xi Wang</b>, <a href="https://www.cv.tu-berlin.de/menue/mitarbeiter/andreas_ley/">Andreas Ley</a>,
                <a href="http://www.cg.tu-berlin.de/team/sebastian-koch/">Sebastian Koch</a>, <a href="http://www.davidlindlbauer.com/">David Lindlbauer</a>,
                <a href="https://www.cc.gatech.edu/~hays/">James Hays</a>, <a href="https://www.researchgate.net/profile/Kenneth_Holmqvist">Kenneth Holmqvist</a>, and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <a href="./project_mental_imagery/retrieval.html">Project page</a> &nbsp;&nbsp;&nbsp;
                <a href="files/papers/mi.pdf">PDF</a> &nbsp;&nbsp;&nbsp;<a href="files/bib/chi19.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/sa18m.png" alt="Tracking the Gaze on Objects in 3D" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Tracking the Gaze on Objects in 3D: How do People Really Look at the Bunny? <small>SIGGRAPH Asia, 2018</small>
                </h4>
                <b>Xi Wang</b>, <a href="http://www.cg.tu-berlin.de/team/sebastian-koch/">Sebastian Koch</a>, <a href="https://www.researchgate.net/profile/Kenneth_Holmqvist">Kenneth Holmqvist</a>, and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <a href="./project_saliency/3D_dataset.html">Project page</a> &nbsp;&nbsp;&nbsp;
                <a href="files/papers/sd.pdf">PDF</a> &nbsp;&nbsp;&nbsp;<a href="files/bib/sa18.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/mvi.png" alt="Maps of Visual Importance" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Maps of Visual Importance: What is recalled from visual episodic memory? <small>arXiv and talk at European Conference on Visual Perception (ECVP), 2018</small>
                </h4>
                <b>Xi Wang</b>, <a href="https://www.researchgate.net/profile/Kenneth_Holmqvist">Kenneth Holmqvist</a>, and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <a href="https://arxiv.org/abs/1712.02142">PDF</a> &nbsp;&nbsp;<a href="files/papers/ecvp18.pdf">Abstract</a> &nbsp;&nbsp;&nbsp;<a href="files/papers/ecvp2018.pptx">Slides</a> &nbsp;&nbsp;&nbsp;
                <a href="files/bib/im17.bib">BibTeX</a>  &nbsp;&nbsp;&nbsp;
            </div>
        </div>
        <p></p>
        <br>


        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/mapping.png" alt="3D Eye Tracking in Monocular and Binocular Conditions" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    3D Eye Tracking in Monocular and Binocular Conditions <small>19th European Conference on Eye Movements (ECEM), 2017</small>
                </h4>
                <b>Xi Wang</b>,<a href="http://www.cognition.tu-berlin.de/menue/tubvision/people/marianne_maertens/"> Marianne Maertens</a> and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <a href="files/papers/ecem17.pdf">Abstract</a> &nbsp;&nbsp;&nbsp;<a href="files/papers/ecem17.pptx">Slides</a> &nbsp;&nbsp;&nbsp;<a href="files/bib/ecem2017.bib">BibTeX</a>
            </div>
        </div>
        <p></p>
        <br>

		<div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/cg&a2016.png" alt="Measuring Visual Salience of 3D Printed Objects" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Measuring Visual Salience of 3D Printed Objects <small>IEEE Computer Graphics and Applications, 2016</small>
                </h4>
                <b>Xi Wang</b>, <a href="http://davidlindlbauer.com/">David Lindlbauer</a>, <a href="http://isgwww.cs.uni-magdeburg.de/graphics//">Christian Lessig</a>, <a href="http://www.cognition.tu-berlin.de/menue/tubvision/people/marianne_maertens/">Marianne Maertens</a> and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <a href="./project_saliency/visual_saliency.html">Project page</a> &nbsp;&nbsp;&nbsp;
                <a href="http://ieeexplore.ieee.org/document/7478427/?arnumber=7478427&newsearch=true&queryText=Measuring%20Visual%20Salience%20of%203D%20Printed%20Objects">PDF</a> &nbsp;&nbsp;&nbsp;
                <a href="files/bib/cg&a2016.bib">BibTeX</a>
            </div>
        </div>
        <p></p>            
        <br>
            
        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/etvis2015.png" alt="Accuracy of Monocular Gaze Tracking on 3D Geometry" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Accuracy of Monocular Gaze Tracking on 3D Geometry <small>Workshop on Eye Tracking and Visualization (ETVIS), 2015 and Book Chapter in Eye Tracking and Visualization</small>
                </h4>
                <b>Xi Wang</b>, <a href="http://davidlindlbauer.com/">David Lindlbauer</a>, <a href="http://isgwww.cs.uni-magdeburg.de/graphics//">Christian Lessig</a> and <a href="http://www.cg.tu-berlin.de/team/prof-dr-marc-alexa/">Marc Alexa</a><br>
                <a href="./project_saliency/gaze_tracking.html">Project page</a>&nbsp;&nbsp;&nbsp;
                <a href="files/papers/etvis15_wang.pdf">PDF</a>&nbsp;&nbsp;&nbsp;<a href="files/papers/etvis_book_wang.pdf">Chapter</a> &nbsp;&nbsp;&nbsp;
                <a href="https://link.springer.com/chapter/10.1007/978-3-319-47024-5_10">Book</a>&nbsp;&nbsp;&nbsp; <a href="files/bib/etvis2015.bib">BibTeX</a>
            </div>
        </div>            
        <p></p>            
        <br>
            
        <div class="row">
            <div class="col-md-3">
                <img class="hidden-phone img-polaroid2" src="imgs/igarss2014.png" alt="Graph-cut Segmentation of Polarimetric SAR Images" style="width: 180px;">
            </div>
            <div class="col-md-9">
                <h4>
                    Graph-cut Segmentation of Polarimetric SAR Images. <small>IEEE Geoscience and Remote Sensing Symposium, 2014</small>
                </h4>                
                    <a href="http://www.rhaensch.de/">Ronny Haensch</a>, <a href="http://www.cv.tu-berlin.de/menue/mitarbeiter/olaf_hellwich/">Olaf Hellwich</a>, and <b>Xi Wang</b><br>
                    <a href="http://ieeexplore.ieee.org/document/7478427/?arnumber=7478427&newsearch=true&queryText=Measuring%20Visual%20Salience%20of%203D%20Printed%20Objects">PDF</a>&nbsp;&nbsp;&nbsp;
                    <a href="files/bib/igarss2014.bib">BibTeX</a>
            </div>
        </div>
        <p></p>            
        <br>
            
		<div class="row">
			<div class="col-md-3">
				<img class="hidden-phone img-polaroid2" src="imgs/visapp2014.png" alt="Color Spaces for Image Segmentation Using Graph-cuts" style="width: 180px;">
			</div>
			<div class="col-md-9">
				<h4>
					Comparison of Different Color Spaces for Image Segmentation Using Graph-cuts. <small>Computer Vision Theory and Application (VISAPP), 2014</small>
				</h4>

                <b>Xi Wang</b>, <a href="http://www.rhaensch.de/">Ronny Haensch</a>, <a href="http://www.cs.sjtu.edu.cn/en/PeopleDetail.aspx?id=154">Lizhuang Ma</a>, and <a href="http://www.cv.tu-berlin.de/menue/mitarbeiter/olaf_hellwich/">Olaf Hellwich</a><br>
					<a href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0004681603010308">PDF</a>&nbsp;&nbsp;&nbsp; <a href="files/bib/visapp2014.bib">BibTeX</a>
			</div>
		</div>
		<p></p>                        
		<br>
            
		<div class="row">
			<div class="col-md-3">
				<img class="hidden-phone img-polaroid2" src="imgs/icip2012.png" alt="Depth Image-based Rendering" style="width: 180px;">
			</div>
			<div class="col-md-9">
				<h4>
					Depth Image-based Rendering with Spatio-temporally Consistent Texture Synthesis for 3D Video with Global Motion. <small>IEEE International Conference on Image Processing (ICIP), 2012</small>
				</h4>
					<a href="http://iphome.hhi.de/koeppel/">Martin Koeppel</a>, <b>Xi Wang</b>, Dimitar Doshkov, <a href="http://iphome.hhi.de/wiegand/">Thomas Wiegand</a>, and <a href="http://iphome.hhi.de/ndjiki/index.htm">Patrick Ndjiki-Nya</a><br>
					<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.724.6986&rep=rep1&type=pdf">PDF</a>&nbsp;&nbsp;&nbsp; <a href="files/bib/icip2012.bib">BibTeX</a>
			</div>
		</div>
		<p></p>                        
		<br>
		
		<div class="row">
			<div class="col-md-3">
				<img class="hidden-phone img-polaroid2" src="imgs/mmsp2012.png" alt="Consistent Spatio-temporal Filling of Disocclusions" style="width: 180px;">
			</div>
			<div class="col-md-9">
				<h4>
					Consistent Spatio-temporal Filling of Disocclusions in the Multiview-Video-Plus-Depth Format. <small>IEEE International Workshop on Multimedia Signal Processing (MMSP), 2012</small>
				</h4>
					<a href="http://iphome.hhi.de/koeppel/">Martin Koeppel</a>, <b>Xi Wang</b>, Dimitar Doshkov, <a href="http://iphome.hhi.de/wiegand/">Thomas Wiegand</a>, and <a href="http://iphome.hhi.de/ndjiki/index.htm">Patrick Ndjiki-Nya</a><br>
					<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.724.8729&rep=rep1&type=pdf">PDF</a>&nbsp;&nbsp;&nbsp; <a href="files/bib/mmsp2012.bib">BibTeX</a>
			</div>
		</div>
		
		<hr class="featurette-divider"  id="personal"/>
		<h3 class="featurette-heading">Personal</h3>
		<p>
			I was born and grew up in a small city (with only a bit more than 1 million people) in China.
			Five years of my life was spent in Shanghai for study and I had a wonderful time living in Berlin while pursuing my PhD.
			I enjoy travelling and taking <a href="https://500px.com/nicolexiwang">pictures</a> on the way.
			I also enjoy playing guitar and have been in a rock band during my studies in Shanghai.
			Since 2014 I started playing jazz, and discovered the powerful secret of playing jazz -- no matter how it sounds, you can always entitle it as improvisation.
            My favorite jazz guitarists are <a href="http://www.patmetheny.com/">Pat Metheny</a> and <a href="http://billfrisell.com/new-releases/">Bill Frisell</a> (their music can easily give me goosebumps).
			The rest of time is spent cooking, learning to play drum-sets and tap dancing, a perfect combination of jazz and rhythm ;).
		</p>
		<br>

    </div> <!-- /container -->

	<footer></footer>  
	<br>
	<br>
	<br>
	<br>

<!--    &lt;!&ndash; Bootstrap core JavaScript-->
<!--    ================================================== &ndash;&gt;-->
<!--    &lt;!&ndash; Placed at the end of the document so the pages load faster &ndash;&gt;-->
<!--    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>-->
<!--    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>-->
<!--    <script src="../../dist/js/bootstrap.min.js"></script>-->
<!--    &lt;!&ndash; IE10 viewport hack for Surface/desktop Windows 8 bug &ndash;&gt;-->
<!--    <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>-->
  </body>
</html>
